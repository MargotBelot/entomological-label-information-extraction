{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nPS_OR0jwanu"
      },
      "source": [
        "# Detecto Object Detection Tutorial\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8SXSmeH9dkMG"
      },
      "source": [
        "**See tutorials:**\n",
        "- https://www.analyticsvidhya.com/blog/2021/06/simplest-way-to-do-object-detection-on-custom-datasets/\n",
        "\n",
        "- https://github.com/alankbi/detecto\n",
        "\n",
        "**and**\n",
        "- https://towardsdatascience.com/build-a-custom-trained-object-detection-model-with-5-lines-of-code-713ba7f6c0fb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "as1jQkeYxhr6"
      },
      "source": [
        "Segmentation in the context of computer vision refers to the process of dividing an image into multiple segments or regions. Object segmentation, in particular, involves identifying and delineating individual objects within an image. Detecto is an open-source Python library designed for object detection tasks. Detecto is built on top of PyTorch, a popular open-source deep learning library.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_Mc4mG9xWIzh"
      },
      "source": [
        "**About detecto**: https://detecto.readthedocs.io/en/latest/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-CIV1kTAdLwE"
      },
      "source": [
        "# Install PyTorch and Detecto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh6SvBgOQdfp"
      },
      "outputs": [],
      "source": [
        "#!pip3 install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucoMcdA2RGTB",
        "outputId": "37c75710-d9bd-4e60-debb-5b1e9e02baf0"
      },
      "outputs": [],
      "source": [
        "# Check whether your computer has a CUDA-enabled GPU - to increase computing performance\n",
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzoNFuWkROG-",
        "outputId": "7ae1411e-0645-4b35-cfe2-27f40458878b"
      },
      "outputs": [],
      "source": [
        "#!pip install detecto"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_R5jgOWEeIVZ"
      },
      "source": [
        "# Import librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQfbdRsXRRYo"
      },
      "outputs": [],
      "source": [
        "# Detecto\n",
        "from detecto import core, utils, visualize\n",
        "from detecto.visualize import show_labeled_image, plot_prediction_grid\n",
        "from detecto.utils import read_image\n",
        "\n",
        "# If you use Google Colab\n",
        "from google.colab import drive\n",
        "\n",
        "# TorchVision\n",
        "from torchvision import transforms\n",
        "\n",
        "# Third-Party Librairy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import glob, os\n",
        "import pandas as pd\n",
        "import plotly.express as px"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3KnjKayIeeLT"
      },
      "source": [
        "# Path to target folders in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSk1Nl5ZF6Y8",
        "outputId": "a6637668-a79d-4590-9ff5-8368f9c303b3"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('Path to your training data repository')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "buwsA36IeObg"
      },
      "source": [
        "## Custom image augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIU4WpzURWWm"
      },
      "outputs": [],
      "source": [
        "\"\"\"Defines a comprehensive data transformation pipeline using PyTorch's torchvision.transforms module.\n",
        "The pipeline consists of a series of image transformations commonly employed in deep learning tasks,\n",
        "particularly for convolutional neural networks (CNNs) and image datasets.\"\"\"\n",
        "\n",
        "# Compose the transformations into a sequential pipeline using transforms.Compose.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Convert the input image (assumed to be a Tensor) to a PIL (Python Imaging Library) image.\n",
        "    transforms.Resize(900),   # Resize the image to the specified size (900 in this case).\n",
        "    transforms.RandomHorizontalFlip(0.5),  # Apply a random horizontal flip with a probability of 0.5 for data augmentation.\n",
        "    transforms.ColorJitter(saturation=0.2),  # Randomly adjust brightness, contrast, saturation, and hue for further data augmentation.\n",
        "    transforms.ToTensor(),  # Convert the PIL image to a PyTorch Tensor and normalize pixel values to the range [0.0, 1.0].\n",
        "    utils.normalize_transform(),  # Apply additional normalization using a function defined in the utils module.\n",
        "])\n",
        "\n",
        "\"\"\"The resulting 'transform' is designed to be applied to input images during the data loading process,\n",
        "preparing them for consumption by a deep learning model. The order of transformations is crucial,\n",
        "and they are applied sequentially as specified in the transforms.Compose list.\"\"\"\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9a14RELL-HZI"
      },
      "source": [
        "## Change the dataformat from xml_files to csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "1-8eBWZT-EOh",
        "outputId": "14c45efc-fa45-4511-b4bb-186847c12902"
      },
      "outputs": [],
      "source": [
        "# Invokes the 'xml_to_csv' function with the path 'train_xml/' representing the directory\n",
        "# containing XML files for training data and 'train_labels.csv' as the desired CSV file name.\n",
        "utils.xml_to_csv('train_xml/', 'train_labels.csv')\n",
        "\n",
        "# The second line does the same for the validation data. It calls 'xml_to_csv' with 'val_xml/' as the\n",
        "# directory containing XML files for validation data and 'val_labels.csv' as the target CSV file name.\n",
        "utils.xml_to_csv('val_xml/', 'val_labels.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f-p4YOvJeupM"
      },
      "source": [
        "# Model Training and saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWo0e3zUGVJz"
      },
      "outputs": [],
      "source": [
        "\"\"\"Create a training dataset using the 'core.Dataset' class. The dataset is initialized with the path to a CSV file\n",
        "containing annotations for the training images ('train_labels.csv'), the directory containing the training images\n",
        "('train_jpg/'), and a data transformation pipeline called 'transform' which includes image preprocessing steps.\"\"\"\n",
        "Train_dataset = core.Dataset(\"train_labels.csv\", \"train_jpg/\", transform=transform)\n",
        "\n",
        "\"\"\"Create a validation dataset using the 'core.Dataset' class. Similar to the training dataset, it is initialized with\n",
        "the path to a CSV file containing annotations for the validation images ('val_labels.csv'), the directory containing\n",
        "the validation images ('val_jpg/'), and the same data transformation pipeline 'transform.'\"\"\"\n",
        "val_dataset = core.Dataset(\"val_labels.csv\", \"val_jpg/\", transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPRTZDJzGjun"
      },
      "outputs": [],
      "source": [
        "train_loader = core.DataLoader(Train_dataset, batch_size=2, shuffle=True)# DataLoader for train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1aTiED8G-Wy",
        "outputId": "203df9ff-d795-49f1-8a87-e72a02d01a3c"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of training samples: {len(Train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\\n\")\n",
        "model = core.Model([\"label\"]) # Classes in our dataset\n",
        "#model.get_internal_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6ufsVU8kd48k",
        "outputId": "e663ef2a-b3d3-4673-f749-d5a1f69363ad"
      },
      "outputs": [],
      "source": [
        "# Train the model using the 'fit' method, providing the training data loader ('train_loader'),\n",
        "# validation dataset ('val_dataset'), and additional training parameters.\n",
        "losses = model.fit(train_loader, val_dataset, epochs=10, lr_step_size=5, learning_rate=0.001, verbose=True)\n",
        "\n",
        "# Print a separator line and then save the trained model with the filename \"model_segmentation_label.pth.\"\n",
        "print(\"-----------\\n\")\n",
        "model.save(\"model_segmentation_label.pth\") # Name the model\n",
        "print('SAVING MODEL COMPLETE...\\n')\n",
        "print(\"-----------\\n\")\n",
        "\n",
        "# Plot the training and validation losses over the epochs using matplotlib.\n",
        "epochs = range(0, 10)\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.plot(epochs, losses, 'b')  # Plot the losses in blue.\n",
        "plt.title('Training and Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()  # Display the legend (assuming there are multiple curves to be shown).\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3Auhl0Ki-pUA"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiOedyykjIoJ",
        "outputId": "2dec8c3a-5d6c-41f2-85cb-4ed5538d78ad"
      },
      "outputs": [],
      "source": [
        "\"\"\"Load a pre-trained model from the file \"model_segmentation_label.pth\" using the 'load' method\n",
        "from the 'core.Model' class. The second argument ([\"label\"]) suggests that the model is loaded\n",
        "with a specific set of class labels; the labels have to be listed in the same order as they were listed in the line above \"model = core.Model([\"label\"])\"\"\"\n",
        "\n",
        "model = core.Model.load(\"Path to the directory where the model is saved\", [\"label\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wvQQcXYHtFp5"
      },
      "source": [
        "# Test Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZiXB33UtIxH",
        "outputId": "f2cbb862-70c6-4632-a9e4-c654a8889635"
      },
      "outputs": [],
      "source": [
        "# Specify the path to your test images\n",
        "image = utils.read_image('test/00acfab7-6f73-40bc-a209-599aee67f795_label_front_0004_label.jpg')\n",
        "predictions = model.predict(image)\n",
        "\n",
        "labels, boxes, scores = predictions\n",
        "print(labels) # Class\n",
        "print(boxes) # Coordinates\n",
        "print(scores) # Prediction scores between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "G8P4oLzWeV1u",
        "outputId": "188f4c0e-adfc-4229-a673-a0e0e00f4969"
      },
      "outputs": [],
      "source": [
        "show_labeled_image(image, boxes, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0Igtyg1emuc"
      },
      "outputs": [],
      "source": [
        "# Set the threshold value between 0 and 1 for filtering the prediction scores.\n",
        "# Only predictions with confidence scores greater than 'thresh' will be considered.\n",
        "thresh = 0.8\n",
        "\n",
        "# Use NumPy to find the indices where the confidence scores exceed the threshold.\n",
        "filtered_indices = np.where(scores > thresh)\n",
        "\n",
        "# Extract the filtered scores, boxes, and labels based on the indices obtained above.\n",
        "filtered_scores = scores[filtered_indices]\n",
        "filtered_boxes = boxes[filtered_indices]\n",
        "num_list = filtered_indices[0].tolist()\n",
        "filtered_labels = [labels[i] for i in num_list]\n",
        "\n",
        "# Display the image with the filtered bounding boxes and corresponding labels.\n",
        "show_labeled_image(image, filtered_boxes, filtered_labels)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DvZ_nEzLrCa5"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c0tdcSP-9nuY",
        "outputId": "1127ce3d-d828-4569-b821-0d9c7d128ed2"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to store predictions for all images.\n",
        "all_predictions = []\n",
        "\n",
        "# Iterate over each file in the \"test/\" directory with a \".jpg\" extension.\n",
        "for file in glob.glob(f\"test/*.jpg\"):\n",
        "    # Read the image using the 'read_image' function from the 'utils' module.\n",
        "    image = utils.read_image(file)\n",
        "    # Perform predictions using the pre-trained segmentation model ('model').\n",
        "    predictions = model.predict(image)\n",
        "    labels, boxes, scores = predictions\n",
        "    # Set the threshold value for filtering predictions.\n",
        "    thresh = 0.8\n",
        "    # Use NumPy to find the indices where the confidence scores exceed the threshold.\n",
        "    filtered_indices = np.where(scores > thresh)\n",
        "    # Extract the filtered scores, boxes, and labels based on the indices obtained above.\n",
        "    filtered_scores = scores[filtered_indices]\n",
        "    filtered_boxes = boxes[filtered_indices]\n",
        "    num_list = filtered_indices[0].tolist()\n",
        "    filtered_labels = [labels[i] for i in num_list]\n",
        "    # Visualize the labeled image with the filtered bounding boxes and corresponding labels.\n",
        "    show_labeled_image(image, filtered_boxes, filtered_labels)\n",
        "    # Store predictions for the current image in the 'all_predictions' list.\n",
        "    all_predictions.append(predictions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save Predictions in CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u9s08-oPPCBG"
      },
      "outputs": [],
      "source": [
        "def create_predictions(model,path):\n",
        "  \"\"\"\n",
        "  Uses the trained model created by Detecto and tries to predict boxes of all files\n",
        "  in a directory. It then returns a pandas Dataframe\n",
        "\n",
        "  Args:\n",
        "            model(detecto.core.Model): model created with detecto\n",
        "            path (str): path where the pictures are located\n",
        "\n",
        "  Returns:\n",
        "            DataFrame: pandas Dataframe with the results\n",
        "\n",
        "  \"\"\"\n",
        "  all_predictions = []\n",
        "  print(\"Predicting coordinates\")\n",
        "  for file in glob.glob(f\"{path}/*.jpg\"):\n",
        "    # Specify the path to your image\n",
        "    image = utils.read_image(file)\n",
        "    predictions = model.predict(image)\n",
        "    # Predictions format: (labels, boxes, scores)\n",
        "    labels, boxes, scores = predictions\n",
        "    for i, labelname in enumerate(labels):\n",
        "      entry = {}\n",
        "      entry['filename'] = os.path.basename(file) # Gets the filename without the dir\n",
        "      entry['class'] = labelname\n",
        "      entry['score'] = scores[i]\n",
        "      entry['xmin'] = boxes[i][0]\n",
        "      entry['ymin'] = boxes[i][1]\n",
        "      entry['xmax'] = boxes[i][2]\n",
        "      entry['ymax'] = boxes[i][3]\n",
        "      all_predictions.append(entry)\n",
        "  df = pd.DataFrame(all_predictions)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gcv1g5_c1YK"
      },
      "outputs": [],
      "source": [
        "df = create_predictions(model, \"test/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yXA93cDZ1iS"
      },
      "outputs": [],
      "source": [
        "def get_clean_dataframe(dataframe, threshold = 0.8):\n",
        "  \"\"\"\n",
        "  Creates a clean dataframe only with boxes exceeding a given threshold score.\n",
        "\n",
        "  Args:\n",
        "            dataframe(pandas.DataFrame): pandas Dataframe with predicted labels\n",
        "            threshold(int): threshold value for scores\n",
        "\n",
        "    Returns:\n",
        "            DataFrame: pandas Dataframe with the trimmed results\n",
        "\n",
        "  \"\"\"\n",
        "  df = dataframe\n",
        "  # Clean the data digits\n",
        "  colnames = ['score','xmin', 'ymin', 'xmax', 'ymax']\n",
        "  for header in colnames:\n",
        "    # Trimm the cells so that they only contain integers\n",
        "    df[header] = df[header].astype('str').str.extractall('(\\d+.\\d+)').unstack().fillna('').sum(axis=1).astype(float)\n",
        "\n",
        "  df = df.loc[ df['score'] >= threshold ] # Keep only rows where the score exceeds the threshold\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp4AXcI4tpwh"
      },
      "outputs": [],
      "source": [
        "df = get_clean_dataframe(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRv-icn_wYkp"
      },
      "outputs": [],
      "source": [
        "df.to_csv('predicted.csv') # Save Clean Dataframe with Threshold"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
